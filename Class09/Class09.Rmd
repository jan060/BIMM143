---
title: "Class09"
author: "Julie Nguyen"
date: "October 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###PCA Objectives
* To reduce dimensionality
* To visualize multidimensional data
* To choose the most useful variables (features)
* To identify groupings of objects (e.g. genes/samples)
* To identify outliers

#Unsupervised Learning Analysis of Human Breast Cancer Cells

##1. Exploratory Data Analysis

Prepare the data of the biopsy results from patients.
```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
```

We have `r nrow(wisc.df)` samples in this dataset.
The `id` and `diagnosis` columns will not be used for most of the following steps.

How many benign and malignant samples do we have in the dataset?
```{r}
table(wisc.df$diagnosis)
```

Convert the features of the data to a matrix.
```{r}
#Convert the features of the data excluding the first two columns and the very last column.
wisc.data <- as.matrix(wisc.df[, 3:32])

#Set the rownames of wisc.data.
row.names(wisc.data) <- wisc.df$id

#View wisc.data.
head(wisc.data)
```

Set the `diagnosis` for future reference as a separate vector.
```{r}
diagnosis <- wisc.df$diagnosis
```

###Questions:

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.df)
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
```

Q3. How many variables/features in the data are suffixed with _mean?
```{r}
mean_variables <- grep("_mean", colnames(wisc.df), value = TRUE)
length(mean_variables)
```

##2. Principal Component Analysis

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled.
```{r}
round(colMeans(wisc.data), 3)
```

```{r}
#Apply the standard deviation function over the columns.
round(apply(wisc.data, 2, sd), 3)
```

These values look very different so I will use `scale=TRUE` when I run PCA.

Perform PCA on wisc.data.
```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)

#Look at the summary of the results.
summary(wisc.pr)
```

Let's make a plot of PC1 vs. PC2. Color by malignant/benign.
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 2], 
     col = diagnosis, 
     xlab = "PC2", ylab = "PC1")
```

###Questions:

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

A. PC1 captures 44.27% of the original variance.

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

A. 3 PCs are required to descirbe at least 70% of the original variance.

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

A. 7 PCs are required to describe at least 90% of the original variance.

Generate a similar plot for PC1 and PC3. What do you notic about these plots?
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 3], col = diagnosis,
     xlab = "PC3", ylab = "PC1")
```

##3. Hierarchical Clustering

Scale the `wisc.data` and assign the results to `data.scaled`.
```{r}
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to `data.dist`.
```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to `wisc.hclust`.
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

###Results of hierarchical clustering
Let’s use the hierarchical clustering model you just created to determine a height (or distance between clusters) where a certain number of clusters exists. What is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```

###Selecting number of clusters
In this section, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature. 

Use `cutree()` to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

We can use the `table()` function to compare the cluster membership to the actual diagnoses.
```{r}
table(wisc.hclust.clusters, diagnosis)
```

##4. K-means clustering
###K-means clustering and comparing results
In this section, you will create a k-means clustering model on the Wisconsin breast cancer data and compare the results to the actual diagnoses and the results of your hierarchical clustering model.

Create a k-means model on `wisc.data`, assigning the result to `wisc.km`. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data (with the `scale()` function) and repeat the algorithm 20 times (by setting setting the value of the `nstart` argument appropriately). Running multiple times such as this will help to find a well performing model.
```{r}
wisc.km <- kmeans(scale(wisc.data), centers = 2, nstart = 20)
```

Use the `table()` function to compare the cluster membership of the k-means model (wisc.km$cluster) to the actual diagnoses contained in the diagnosis vector.
```{r}
table(wisc.km$cluster, diagnosis)
```

Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

A. K-means separates the 2 diagnoses similarly to hclust. Group 1 is mostly malignant patients. Group 2 is mostly benign patients. 

Use the `table()` function to compare the cluster membership of the k-means model (`wisc.km$cluster`) to your hierarchical clustering model from above (`wisc.hclust.clusters`).
```{r}
table(wisc.km$cluster, wisc.hclust.clusters)
```

##5. Combining methods
###Clustering on PCA results
Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage `method="ward.D2"`.
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

Inspect the results from k-means clustering of 2.
```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```

Create a table that separates the groups into their diagnoses.
```{r}
table(grps, diagnosis)
```

Now plot PC1 vs. PC2 and color the points by their groups.
```{r}
plot(wisc.pr$x[, 1:2], col = grps)
```

Compare the plot above to the plot colored by diagnoses.
```{r}
plot(wisc.pr$x[, 1:2], col = diagnosis)
```

We cluster the data into 2 groups instead of by diagnosis because notice how group 1 is composed of mostly malignant patients while group 2 is composed of mostly benign. Benign patients in group 1 may be false negatives while malignant patients in group 2 may be false positives.

Note the color swap here as the `hclust` cluster 1 is mostly “M” and cluster 2 is mostly “B” as we saw from the results of calling `table(grps, diagnosis)`. To match things up we can turn our groups into a factor and reorder the levels so cluster 2 comes first.
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
#malignant = black, group 1
#benign = red, group 2
```

##7. Prediction
We will use the `predict()` function that will take our PCA model from before and the "new cancer cell data" and project that data onto our PCA space.
```{r}
new <- read.csv("new_samples.csv")
npc <- predict(wisc.pr, newdata = new)
npc
```

Now plot the new data onto the existing `wisc.data`.
```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q17. Which of these new patients should we prioritize for follow up based on your results?

A. Patient 1 should be prioritized.